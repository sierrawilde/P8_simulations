{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the cluster, the simulations can't be outputed in subdirectories according to slope, and thus loading the data into json files requires a different process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n",
      "        _                __     \n",
      "  _____(_)________ _____/ /___ _\n",
      " / ___/ / ___/ __ `/ __  / __ `/\n",
      "/ /__/ / /__/ /_/ / /_/ / /_/ / \n",
      "\\___/_/\\___/\\__,_/\\__,_/\\__,_/  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21T21:45:04\u001b[32m[INFO    ] CicadaPy(50) -> \u001b[35mAll set!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import ROOT as r\n",
    "import os\n",
    "from ReadKTOutputFile import *\n",
    "from root_numpy import tree2array\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadFilelist(mypath, search_str = '.'):\n",
    "    '''\n",
    "    Get list of all files fullfilling a few naming conditions\n",
    "    '''\n",
    "    filelist = []\n",
    "    print('Searching files in {} with \"{}\" in filename'.format(mypath, search_str))\n",
    "    for (dirpath, dirnames, filenames) in os.walk(mypath):\n",
    "        for name in filenames:\n",
    "            if '.root' in name and search_str in name:\n",
    "                filelist.append(name)\n",
    "            elif '.json' in name and search_str in name:\n",
    "                filelist.append(name)\n",
    "    return filelist\n",
    "\n",
    "def GetEventIdFromFilename(filename):\n",
    "    splitted_name = filename.replace('.', '_')\n",
    "    splitted_name = splitted_name.split('_')\n",
    "    for s in splitted_name:\n",
    "        #print(s)\n",
    "        if s.isdigit():\n",
    "            break\n",
    "    return s\n",
    "\n",
    "# get list of files\n",
    "\n",
    "def get_path_list(paths, slopes):\n",
    "    list = []\n",
    "    for path in paths:\n",
    "        for slope in slopes:\n",
    "            list.append(path + str(slope))\n",
    "    return list\n",
    "\n",
    "def read_root(path_to_sim, branch_name):\n",
    "    f = r.TFile.Open(path_to_sim, 'read')\n",
    "    \n",
    "    tree=f.Get(\"Event_0\")\n",
    "    \n",
    "    start_freq_true = tree2array(tree, branches=[branch_name])[0][0]\n",
    "    \n",
    "    return start_freq_true\n",
    "\n",
    "# get the list of slopes from the name of the root files\n",
    "# assuming a file name with the format: '[type]_event_[slope]_[simnumber].root'\n",
    "# if a different format is used then this function will need to be changed\n",
    "\n",
    "def get_slope(event_file):\n",
    "    pre_index = (event_file.find('_'))+1\n",
    "    first_index = event_file.find('_', pre_index)+1\n",
    "    last_index = event_file.find('_', first_index)\n",
    "    slope = float(event_file[first_index:last_index])\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching files in ../Testing_1directory_slopescan19/ with \"reconstructed_event\" in filename\n",
      "Searching files in ../Testing_1directory_slopescan19/ with \"simulated_event\" in filename\n",
      "Searching files in ../Testing_1directory_slopescan19/ with \"snr_and_power_and_slope\" in filename\n",
      "Searching files in ../Testing_1directory_slopescan20/ with \"reconstructed_event\" in filename\n",
      "Searching files in ../Testing_1directory_slopescan20/ with \"simulated_event\" in filename\n",
      "Searching files in ../Testing_1directory_slopescan20/ with \"snr_and_power_and_slope\" in filename\n"
     ]
    }
   ],
   "source": [
    "#CHANGE THIS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "list_of_directories = ['../Testing_1directory_slopescan19/', '../Testing_1directory_slopescan20/']\n",
    "# list_of_slopes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6,\n",
    "#                   0.7, 0.8, 0.9, 1.0, 1.1]\n",
    "path_list = list_of_directories\n",
    "#path_list = get_path_list(list_of_directories, list_of_slopes)\n",
    "TrackAndEventFileList = []\n",
    "SimulatedEventFileList = []\n",
    "SNRFileList = []\n",
    "k_index = []\n",
    "simulated_snr = []\n",
    "simulated_snr_event = []\n",
    "simulated_slope = []\n",
    "simulated_slope_event = []\n",
    "start_frequencies = []\n",
    "true_start_frequencies = []\n",
    "true_track_times = []\n",
    "reconstructed_track_times = []\n",
    "for path in path_list:\n",
    "    #Track and event file and simulated event file\n",
    "    TrackAndEventFileList.append(sorted(LoadFilelist(path, 'reconstructed_event')))\n",
    "    SimulatedEventFileList.append(sorted(LoadFilelist(path, 'simulated_event')))\n",
    "    SNRFileList.append(sorted(LoadFilelist(path, 'snr_and_power_and_slope')))\n",
    "    #print(SNRFileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['snr_and_power_and_slope_0.1.json', 'snr_and_power_and_slope_0.4.json', 'snr_and_power_and_slope_0.7.json', 'snr_and_power_and_slope_1.0.json'], ['snr_and_power_and_slope_0.1.json', 'snr_and_power_and_slope_0.4.json', 'snr_and_power_and_slope_0.7.json', 'snr_and_power_and_slope_1.0.json']]\n"
     ]
    }
   ],
   "source": [
    "print(SNRFileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: no tree multiTrackEvents in file\n",
      "Error: no tree multiTrackEvents in file\n",
      "Error: no tree multiTrackEvents in file\n",
      "Error: no tree multiTrackEvents in file\n",
      "Error: no tree multiTrackEvents in file\n",
      "Error: no tree multiTrackEvents in file\n"
     ]
    }
   ],
   "source": [
    "# check events are in files\n",
    "    \n",
    "for each_slope, each_path in zip(TrackAndEventFileList, path_list):\n",
    "    k_id = []\n",
    "    for i in range(len(each_slope)):\n",
    "        try:\n",
    "            start_times = ReadKTOutputFile(os.path.join(each_path, each_slope[i]), var='StartTimeInRunC')\n",
    "            #print(start_times)\n",
    "        except Exception as e:\n",
    "            #print(e) # you ll probably get an error when no events are present in the file. if you 're sure that is the error, you can comment this line.\n",
    "            continue\n",
    "\n",
    "        file_id = i\n",
    "        if len(start_times) > 0:\n",
    "            k_id.append(file_id)\n",
    "    k_index.append(k_id)\n",
    "\n",
    "    #load reconstructed events' start frequencies\n",
    "    for each in each_slope:\n",
    "        try:\n",
    "            start_frequency = ReadKTOutputFile(os.path.join(each_path, each), var='StartFrequency')\n",
    "            #print(start_frequency)\n",
    "        except Exception as e:\n",
    "            #print(e) #comment out after making sure the code works\n",
    "            continue\n",
    "        if len(start_frequency) != 0:\n",
    "            start_frequencies.append(start_frequency)\n",
    "            \n",
    "    #load json file with simulated snrs\n",
    "    snr_run_list = []\n",
    "    for each_snr in SNRFileList[0]:\n",
    "        snr_and_power_file = each_path + each_snr\n",
    "        with open(snr_and_power_file) as infile:\n",
    "            a = json.load(infile)\n",
    "        snr_run_list.append(a['snr'])\n",
    "    simulated_snr.append(np.asarray(snr_run_list).flatten())\n",
    "    \n",
    "        #print(simulated_snr)\n",
    "        \n",
    "#load the simulated slopes\n",
    "for each_sim in SimulatedEventFileList:\n",
    "    slope_run_list = []\n",
    "    for sim in each_sim:\n",
    "        slope_run_list.append(get_slope(sim))\n",
    "    simulated_slope.append(slope_run_list)\n",
    "\n",
    "#select the simulated slopes and snrs associated with the events successfully reconstructed\n",
    "for each_path, each_sim, k_id, slope, snr in zip(path_list, SimulatedEventFileList, k_index, simulated_slope, simulated_snr):\n",
    "    slope_run_list_event = np.asarray(slope)[k_id]\n",
    "    simulated_slope_event.append(slope_run_list_event.tolist())\n",
    "    snr_run_list_event = np.asarray(snr)[k_id]\n",
    "    simulated_snr_event.append(snr_run_list_event.tolist())\n",
    "        \n",
    "# print(simulated_slope)\n",
    "# print(simulated_slope_event)\n",
    "# print(simulated_snr)\n",
    "# print(simulated_snr_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56409004.40705187], [56318359.37499995], [56030273.43749994], [56416829.42708318]]\n",
      "[[0.1, 0.4], [0.4, 0.4]]\n"
     ]
    }
   ],
   "source": [
    "print(start_frequencies)\n",
    "print(simulated_slope_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all of the lists to arrays\n",
    "simulated_snr = np.asarray(simulated_snr)\n",
    "simulated_snr_event = np.asarray(simulated_snr_event).flatten()\n",
    "simulated_slope = np.asarray(simulated_slope)\n",
    "simulated_slope_event = np.asarray(simulated_slope_event).flatten()\n",
    "reconstructed_freq = np.asarray(start_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load true start frequencies\n",
    "\n",
    "for each_sim, each_path, each_rec in zip(SimulatedEventFileList, path_list, TrackAndEventFileList):\n",
    "    for i in range(len(each_sim)):\n",
    "        try:\n",
    "            true_start_frequencies.append(read_root(os.path.join(each_path, each_sim[i]), 'StartFrequencies'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(each_path+each_sim[i])\n",
    "            true_start_frequencies.append(np.nan)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.59064051e+10]\n"
     ]
    }
   ],
   "source": [
    "print(true_start_frequencies[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load true start times\n",
    "\n",
    "for each_sim, each_path, each_rec in zip(SimulatedEventFileList, path_list, TrackAndEventFileList):\n",
    "    for i in range(len(each_sim)):\n",
    "        try:\n",
    "             true_track_times.append(read_root(os.path.join(each_path, each_sim[i]), 'StartTimes'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(each_path+each_sim[i])\n",
    "            true_track_times.append(np.nan)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(true_track_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load reconstructed start times\n",
    "\n",
    "for each_sim, each_path, each_rec in zip(SimulatedEventFileList, path_list, TrackAndEventFileList):\n",
    "    for i in range(len(each_rec)):\n",
    "        try:\n",
    "            reconstructed_track_times.append(ReadKTOutputFile(os.path.join(each_path, each_rec[i]), 'StartTimeInRunC', objectType='TProcessedTrackData', name='procTracks:Track'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(each_path+each_rec[i])\n",
    "            reconstructed_track_times.append(np.nan)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0057548799999999995], [0.005632, 0.0166912], [0.00595968], [0.0060416], [0.00534528, 0.00546816], [0.00595968], [0.00526336]]\n"
     ]
    }
   ],
   "source": [
    "print(reconstructed_track_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_freq = np.ndarray.tolist((reconstructed_freq+24.5e9+1.4e9-50e6)/1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n",
      "16\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(rec_freq))\n",
    "print(len(true_start_frequencies))\n",
    "print(len(true_track_times))\n",
    "print(len(reconstructed_track_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the true start frequencies and selecting the datapoints that correspond to reconstructed events\n",
    "sub_event_ids = k_index\n",
    "real_freq = []\n",
    "for event in true_start_frequencies:\n",
    "    real_freq.append(np.min(event))\n",
    "\n",
    "true_freq_simulated = np.asarray(real_freq)/1e3\n",
    "\n",
    "true_freq = []\n",
    "true_start_times = []\n",
    "reconstructed_start_times = []\n",
    "for each_run in sub_event_ids:\n",
    "    for i in each_run:\n",
    "        true_freq.append(true_freq_simulated[i])\n",
    "        true_start_times.append(list(true_track_times[i]))\n",
    "        reconstructed_start_times.append(list(reconstructed_track_times[i]))\n",
    "#true_freq = np.asarray(true_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(true_freq_simulated.flatten()))\n",
    "print(len(true_freq))\n",
    "\n",
    "print(len(true_start_times))\n",
    "print(len(reconstructed_start_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_slope_event = simulated_slope_event.tolist()\n",
    "simulated_snr_event = simulated_snr_event.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#storing data in json files\n",
    "with open('../Testing_1directory_slopescan20/true_frequencies.json', 'w') as true_freq_file:\n",
    "    json.dump(true_freq, true_freq_file)\n",
    "with open('../Testing_1directory_slopescan20/reconstructed_frequencies.json', 'w') as rec_freq_file:\n",
    "    json.dump(rec_freq, rec_freq_file)\n",
    "\n",
    "with open('../Testing_1directory_slopescan20/simulated_slope.json', 'w') as sim_slope_file:\n",
    "    json.dump(np.ndarray.tolist(simulated_slope), sim_slope_file)\n",
    "with open('../Testing_1directory_slopescan20/simulated_slope_event.json', 'w') as sim_slope_event_file:\n",
    "    json.dump(simulated_slope_event, sim_slope_event_file)\n",
    "\n",
    "with open('../Testing_1directory_slopescan20/simulated_snr.json', 'w') as sim_snr_file:\n",
    "    json.dump(np.ndarray.tolist(simulated_snr), sim_snr_file)\n",
    "with open('../Testing_1directory_slopescan20/simulated_snr_event.json', 'w') as sim_snr_event_file:\n",
    "    json.dump(simulated_snr_event, sim_snr_event_file)\n",
    "\n",
    "with open('../Testing_1directory_slopescan20/simulated_start_times.json', 'w') as sim_start_times_file:\n",
    "    json.dump(true_start_times, sim_start_times_file)\n",
    "with open('../Testing_1directory_slopescan20/reconstructed_start_times.json', 'w') as rec_start_times_file:\n",
    "    json.dump(reconstructed_start_times, rec_start_times_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-3f70b7c3bd06>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-3f70b7c3bd06>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    reconstructed_slope.append(ReadKTOutput(os.path.join(each_path, each), var=))\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#load reconstructed snr and slope files that are known to have reconstructed events\n",
    "reconstructed_snr = []\n",
    "reconstructed_slope = []\n",
    "for each_path, each_rec in zip(path_list, TrackAndEventFileList):\n",
    "    for each in each_rec:\n",
    "        reconstructed_slope.append(ReadKTOutput(os.path.join(each_path, each), var='firstTrackSlope'))\n",
    "        resonstructed_snr.append(ReadKTOutput(os.path.join(each_path, each), var=))\n",
    "        \n",
    "#do I need to load in the actual simulated frequencies and compare track to track, or just compare the first track slopes and snrs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
